gha-eks-addons/quest-eks-addons/modules/grafana/main.tf

locals {
  grafana_ingress_annotations = merge(
    {
      "kubernetes.io/ingress.class"                 = "alb"
      "alb.ingress.kubernetes.io/scheme"           = var.grafana_alb_scheme          # "internal" or "internet-facing"
      "alb.ingress.kubernetes.io/target-type"      = "ip"
      "alb.ingress.kubernetes.io/healthcheck-path" = "/api/health"
      "alb.ingress.kubernetes.io/success-codes"    = "200-399"
      "alb.ingress.kubernetes.io/healthcheck-port" = "traffic-port"

      # >>> these two make Grafana and Prometheus share the SAME ALB <<<
      "alb.ingress.kubernetes.io/group.name"  = "monitoring"
      "alb.ingress.kubernetes.io/group.order" = "10"
    },
    # DO NOT include certificate-arn here for HTTP-only
    var.grafana_certificate_arn != "" ? {} : {},
    var.grafana_external_dns_hostname != "" ? {
      "external-dns.alpha.kubernetes.io/hostname" = var.grafana_external_dns_hostname
    } : {}
  )
}



helm_release "grafana":



values = [
  yamlencode({
    "grafana.ini" = {
      server = {
        domain   = var.grafana_host
        root_url = "http://${var.grafana_host}"  # HTTP here
      }
    }

    service = { type = "ClusterIP" }

    ingress = {
      enabled          = true
      ingressClassName = "alb"
      annotations      = local.grafana_ingress_annotations
      hosts            = [var.grafana_host]
      path             = "/"
      pathType         = "Prefix"
    }
  })
]





gha-eks-addons/quest-eks-addons/modules/prometheus_stack/variables.tf


variable "prometheus_host" {
  description = "DNS host users will hit for Prometheus (e.g., prometheus.devops-tools.example.com)"
  type        = string
}


gha-eks-addons/quest-eks-addons/modules/prometheus_stack/main.tf



values = [
  yamlencode({
    grafana = {
      enabled = false
    }

    prometheus = {
      ingress = {
        enabled          = true
        ingressClassName = "alb"
        annotations = {
          "kubernetes.io/ingress.class"                 = "alb"
          "alb.ingress.kubernetes.io/scheme"           = var.grafana_alb_scheme      # reuse same scheme
          "alb.ingress.kubernetes.io/target-type"      = "ip"
          "alb.ingress.kubernetes.io/healthcheck-path" = "/-/healthy"
          "alb.ingress.kubernetes.io/success-codes"    = "200"
          "alb.ingress.kubernetes.io/healthcheck-port" = "traffic-port"

          # >>> share the exact same ALB as Grafana <<<
          "alb.ingress.kubernetes.io/group.name"  = "monitoring"
          "alb.ingress.kubernetes.io/group.order" = "20"
        }
        hosts    = [var.prometheus_host]
        paths    = ["/"]
        pathType = "Prefix"
      }
    }
  }),
  file("${path.module}/arc-scrape-config.yaml")
]



gha-eks-addons/quest-eks-addons/tfvars/orion-<ENV>.tfvars

# Existing values you already have...
grafana_host          = "grafana-orion.devops-tools.prd.aws.qdx.com"
grafana_alb_scheme    = "internal"   # or "internet-facing"

# New:
prometheus_host       = "prometheus-orion.devops-tools.prd.aws.qdx.com"

addon.tf


module "prometheus_stack" {
  source                   = "./modules/prometheus_stack"
  namespace                = "monitoring"
  prometheus_chart_repo    = var.prometheus_chart_repo
  prometheus_chart_version = var.prometheus_chart_version

  # ➜ REQUIRED (this is why validate failed)
  prometheus_host          = var.prometheus_host

  # (optional but useful if you want the same ALB behavior as Grafana)
  prometheus_alb_scheme    = var.prometheus_alb_scheme        # "internal" | "internet-facing"
  monitoring_alb_group     = var.monitoring_alb_group          # to share the same ALB as Grafana
}


root:


variable "prometheus_host" {
  description = "DNS host to expose Prometheus on (e.g., prometheus.example.com)"
  type        = string
}

variable "prometheus_alb_scheme" {
  description = "ALB scheme for Prometheus (internal | internet-facing)"
  type        = string
  default     = "internal"
}

variable "monitoring_alb_group" {
  description = "ALB Ingress group name to place Grafana & Prometheus on the same ALB"
  type        = string
  default     = "monitoring"
}

variable "monitoring_alb_group" {
  description = "ALB Ingress group name to share the ALB with Prometheus"
  type        = string
  default     = "monitoring"
}



# replace with your host
curl -i http://grafana-orion.devops-tools.prd.aws.qdx.com/api/health

kubectl -n monitoring get cm -l app.kubernetes.io/name=grafana -o yaml | sed -n '/grafana.ini/,+20p'

kubectl -n monitoring describe ingress grafana

nslookup grafana-orion.devops-tools.prd.aws.qdx.com
nslookup prometheus-orion.devops-tools.prd.aws.qdx.com







prometheus:
  prometheusSpec:
    additionalScrapeConfigs:
      - job_name: "github-arc"
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - action: keep
            source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            regex: "true"

          - action: replace
            source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            target_label: __metrics_path__
            regex: "(.+)"

          # FIX: keep pod IP and set the port from the annotation
          - action: replace
            source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            target_label: __address__
            regex: "([^:]+)(?::\\d+)?;(\\d+)"
            replacement: "$1:$2"



# If any ServiceMonitor/PodMonitor exists for ARC, remove it
kubectl get servicemonitors,podmonitors -A | grep -E 'arc|runner|gha' || true




kubectl get servicemonitors -A | grep -E 'arc|runner|listener|controller' || true

kubectl -n runner-controller patch servicemonitor arc-controller-metrics \
  --type merge -p '{"spec":{"namespaceSelector":{"matchNames":["runner-controller"]}}}'

kubectl -n runner-controller patch servicemonitor arc-listener-metrics \
  --type merge -p '{"spec":{"namespaceSelector":{"matchNames":["runner-controller"]}}}'

kubectl -n monitoring patch servicemonitor arc-controller-metrics-win \
  --type merge -p '{"spec":{"namespaceSelector":{"matchNames":["win-runner-controller"]}}}'

kubectl -n monitoring patch servicemonitor arc-listener-metrics-win \
  --type merge -p '{"spec":{"namespaceSelector":{"matchNames":["win-runner-controller"]}}}'



echo "LINUX:"
kubectl -n runner-controller get servicemonitor arc-controller-metrics -o jsonpath='{.spec.namespaceSelector}'; echo
kubectl -n runner-controller get servicemonitor arc-listener-metrics   -o jsonpath='{.spec.namespaceSelector}'; echo

echo "WINDOWS:"
kubectl -n monitoring get servicemonitor arc-controller-metrics-win -o jsonpath='{.spec.namespaceSelector}'; echo
kubectl -n monitoring get servicemonitor arc-listener-metrics-win   -o jsonpath='{.spec.namespaceSelector}'; echo



# Find the Prometheus server pod (kube-prometheus-stack)
kubectl -n monitoring get pods -l app.kubernetes.io/name=prometheus
# Then delete that pod (it will restart and reload the config)
kubectl -n monitoring delete pod <prometheus-pod-name>


kubectl get pods -A -o wide \
  --show-labels | grep -E 'runner-controller|win-runner-controller|arc|listener|controller'


label_values(gha_desired_runners, actions_github_com_scale_set_name)

sum(gha_assigned_jobs{namespace="$RunnerNamespace", actions_github_com_scale_set_name="$scaleSet"})

rate(gha_job_startup_duration_seconds_sum{actions_github_com_scale_set_namespace="aws-arc-ss-12b9n", actions_github_com_scale_set_runner_namespace="runner-set"}[5m])
/
rate(gha_job_startup_duration_seconds_count{actions_github_com_scale_set_namespace="aws-arc-ss-12b9n", actions_github_com_scale_set_runner_namespace="runner-set"}[5m])



This is expected in Prometheus. Each target shows up once per scrape configuration. If the same pod matches multiple ServiceMonitors, you’ll see it listed more than once. It isn’t a bug — it just means Prometheus is scraping the same pod endpoint under different jobs or selector
