# win-arc-metrics-services.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: arc-controller-metrics
  labels:
    app: arc-controller
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 8080
      targetPort: 8080
  selector:
    app.kubernetes.io/name: gha-rs-controller
---
apiVersion: v1
kind: Service
metadata:
  name: arc-listener-metrics
  labels:
    app: arc-listener
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 8080
      targetPort: 8080
  selector:
    app.kubernetes.io/component: runner-scale-set-listener













# win-arc-servicemonitors.yaml
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: arc-controller-metrics-win
  labels:
    release: kube-prometheus-stack
spec:
  namespaceSelector:
    matchNames: [win-runner-controller]
  selector:
    matchLabels:
      app: arc-controller
  endpoints:
    - port: http-metrics
      path: /metrics
      interval: 30s
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: arc-listener-metrics-win
  labels:
    release: kube-prometheus-stack
spec:
  namespaceSelector:
    matchNames: [win-runner-controller]
  selector:
    matchLabels:
      app: arc-listener
  endpoints:
    - port: http-metrics
      path: /metrics
      interval: 30s




kubectl -n monitoring delete pvc -l app.kubernetes.io/name=grafana --ignore-not-found
kubectl -n monitoring delete pvc -l app.kubernetes.io/name=prometheus --ignore-not-found
kubectl -n monitoring delete pvc -l app.kubernetes.io/name=alertmanager --ignore-not-found







kubectl get sc
# if you do NOT see one marked (default), create it:
cat <<'EOF' | kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp3
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: ebs.csi.aws.com
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
parameters:
  type: gp3
  encrypted: "true"
EOF





NS=monitoring
kubectl -n $NS patch prometheus   kube-prometheus-stack-prometheus   --type=merge -p '{"spec":{"replicas":0}}'
kubectl -n $NS patch alertmanager kube-prometheus-stack-alertmanager --type=merge -p '{"spec":{"replicas":0}}'
kubectl -n $NS scale deploy kube-prometheus-stack-grafana --replicas=0 || true

# wait until no Prom/AM/Grafana pods remain
kubectl -n $NS get pods -w




# list PVCs so we use exact names
kubectl -n $NS get pvc

# example names (adjust to your output):
PROM_PVC=prometheus-kube-prometheus-stack-prometheus-db-prometheus-kube-prometheus-stack-prometheus-0
AM_PVC=alertmanager-kube-prometheus-stack-alertmanager-db-alertmanager-kube-prometheus-stack-alertmanager-0
GRAFANA_PVC=kube-prometheus-stack-grafana

# remove pvc-protection finalizer just in case it blocks deletion
kubectl -n $NS patch pvc $PROM_PVC   --type=merge -p '{"metadata":{"finalizers":[]}}' || true
kubectl -n $NS patch pvc $AM_PVC     --type=merge -p '{"metadata":{"finalizers":[]}}' || true
kubectl -n $NS patch pvc $GRAFANA_PVC --type=merge -p '{"metadata":{"finalizers":[]}}' || true

# delete the PVCs
kubectl -n $NS delete pvc $PROM_PVC --ignore-not-found
kubectl -n $NS delete pvc $AM_PVC   --ignore-not-found
kubectl -n $NS delete pvc $GRAFANA_PVC --ignore-not-found


NS=monitoring

# 1) Make sure the workloads are RUNNING (replicas=1). This creates the “consumer”.
kubectl -n $NS patch prometheus   kube-prometheus-stack-prometheus   --type=merge -p '{"spec":{"replicas":1}}'
kubectl -n $NS patch alertmanager kube-prometheus-stack-alertmanager --type=merge -p '{"spec":{"replicas":1}}'
kubectl -n $NS scale deploy kube-prometheus-stack-grafana --replicas=1

# 2) Watch PVCs bind (the VOLUME column will get a PV name)
kubectl -n $NS get pvc -w



# make sure workloads are not scaled to 0 from earlier steps
kubectl -n monitoring patch prometheus   kube-prometheus-stack-prometheus   --type=merge -p '{"spec":{"replicas":1}}' || true
kubectl -n monitoring patch alertmanager kube-prometheus-stack-alertmanager --type=merge -p '{"spec":{"replicas":1}}' || true
kubectl -n monitoring scale deploy kube-prometheus-stack-grafana --replicas=1 || true




helm -n monitoring status kube-prometheus-stack
helm -n monitoring history kube-prometheus-stack

# pick the last number in the "STATUS  deployed" row
REV=$(helm -n monitoring history kube-prometheus-stack | awk '/deployed/{rev=$1} END{print rev}')
helm -n monitoring rollback kube-prometheus-stack ${REV}


helm -n monitoring uninstall kube-prometheus-stack
# if Terraform still thinks it exists, drop it from state once:
terraform state rm module.prometheus_stack.helm_release.prometheus_stack




NS=monitoring
REL=kube-prometheus-stack

# 1) find the last *deployed* revision
REV=$(helm -n "$NS" history "$REL" | awk '/deployed/{rev=$1} END{print rev}')

# 2) rollback to that good revision (this clears the pending state)
helm -n "$NS" rollback "$REL" "$REV"

# (optional) confirm it's healthy
helm -n "$NS" status "$REL"
kubectl -n "$NS" get pods,pvc,ing -o wide




# username
kubectl -n monitoring get secret kube-prometheus-stack-grafana \
  -o jsonpath='{.data.admin-user}' | base64 -d; echo

# password
kubectl -n monitoring get secret kube-prometheus-stack-grafana \
  -o jsonpath='{.data.admin-password}' | base64 -d; echo



























values = [
  yamlencode({
    grafana = {
      enabled = true
      # Persist Grafana (users, dashboards edited in UI, plugins)
      persistence = {
        enabled          = true
        type             = "statefulset"
        storageClassName = var.grafana_storage_class
        accessModes      = ["ReadWriteOnce"]
        size             = var.grafana_storage_size
      }
      ingress = {
        enabled = true
        ingressClassName = "alb"
        annotations = {
          "kubernetes.io/ingress.class"              = "alb"
          "alb.ingress.kubernetes.io/scheme"         = var.prometheus_alb_scheme
          "alb.ingress.kubernetes.io/target-type"    = "ip"
          "alb.ingress.kubernetes.io/healthcheck-path" = "/login"
          "alb.ingress.kubernetes.io/success-codes"  = "200-399"
          "alb.ingress.kubernetes.io/healthcheck-port" = "traffic-port"
          "alb.ingress.kubernetes.io/group.name"     = var.monitoring_alb_group
          "alb.ingress.kubernetes.io/group.order"    = "20"
        }
        hosts    = [var.grafana_host]
        paths    = ["/"]
        pathType = "Prefix"
      }
    }

    prometheus = {
      ingress = {
        enabled = true
        ingressClassName = "alb"
        annotations = {
          "kubernetes.io/ingress.class"              = "alb"
          "alb.ingress.kubernetes.io/scheme"         = var.prometheus_alb_scheme
          "alb.ingress.kubernetes.io/target-type"    = "ip"
          "alb.ingress.kubernetes.io/healthcheck-path" = "/-/healthy"
          "alb.ingress.kubernetes.io/success-codes"  = "200"
          "alb.ingress.kubernetes.io/healthcheck-port" = "traffic-port"
          "alb.ingress.kubernetes.io/group.name"     = var.monitoring_alb_group
          "alb.ingress.kubernetes.io/group.order"    = "20"
        }
        hosts    = [var.prometheus_host]
        paths    = ["/"]
        pathType = "Prefix"
      }

      # Persist Prometheus TSDB
      prometheusSpec = {
        retention      = var.prometheus_retention
        walCompression = true
        storageSpec = {
          volumeClaimTemplate = {
            spec = {
              storageClassName = var.prometheus_storage_class
              accessModes      = ["ReadWriteOnce"]
              resources = { requests = { storage = var.prometheus_storage_size } }
            }
          }
        }
      }
    }

    alertmanager = {
      enabled = true
      ingress = { enabled = false }

      # Persist Alertmanager state (silences/notifications)
      alertmanagerSpec = {
        storage = {
          volumeClaimTemplate = {
            spec = {
              storageClassName = var.alertmanager_storage_class
              accessModes      = ["ReadWriteOnce"]
              resources = { requests = { storage = var.alertmanager_storage_size } }
            }
          }
        }
      }
    }
  }),
  file("${path.module}/arc-scrape-config.yaml"),
]











# variables.tf

variable "prometheus_storage_class" {
  description = "StorageClass for Prometheus PVC (EBS CSI)."
  type        = string
  default     = "gp3"
}

variable "prometheus_storage_size" {
  description = "Prometheus PVC size."
  type        = string
  default     = "50Gi"
}

variable "prometheus_retention" {
  description = "Prometheus time series retention period."
  type        = string
  default     = "15d"
}

variable "alertmanager_storage_class" {
  description = "StorageClass for Alertmanager PVC (EBS CSI)."
  type        = string
  default     = "gp3"
}

variable "alertmanager_storage_size" {
  description = "Alertmanager PVC size."
  type        = string
  default     = "10Gi"
}

variable "grafana_storage_class" {
  description = "StorageClass for Grafana PVC (EBS CSI)."
  type        = string
  default     = "gp3"
}

variable "grafana_storage_size" {
  description = "Grafana PVC size."
  type        = string
  default     = "10Gi"
}



 prometheus_storage_class   = var.prometheus_storage_class
  prometheus_storage_size    = var.prometheus_storage_size
  prometheus_retention       = var.prometheus_retention
  alertmanager_storage_class = var.alertmanager_storage_class
  alertmanager_storage_size  = var.alertmanager_storage_size
  grafana_storage_class      = var.grafana_storage_class
  grafana_storage_size       = var.grafana_storage_size





























resource "kubectl_manifest" "monitoring_namespace" {
  yaml_body = <<YAML
apiVersion: v1
kind: Namespace
metadata:
  name: ${var.namespace}
  labels:
    kubernetes.io/metadata.name: ${var.namespace}
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/enforce-version: v1.30
    pod-security.kubernetes.io/audit: privileged
    pod-security.kubernetes.io/audit-version: v1.30
    pod-security.kubernetes.io/warn: privileged
    pod-security.kubernetes.io/warn-version: v1.30
YAML
}

resource "helm_release" "prometheus_stack" {
  name             = "kube-prometheus-stack"
  namespace        = var.namespace
  create_namespace = false
  repository       = var.prometheus_chart_repo
  chart            = "actions/prometheus/kube-prometheus-stack"
  version          = var.prometheus_chart_version
  atomic           = true
  wait             = true
  wait_for_jobs    = true
  timeout          = 1200
  disable_openapi_validation = true

  values = [
    yamlencode({
      grafana = {
        enabled = true

        # ---- persistence (EBS gp3) ----
        persistence = {
          enabled          = true
          type             = "statefulset"
          storageClassName = var.grafana_storage_class
          accessModes      = ["ReadWriteOnce"]
          size             = var.grafana_storage_size
        }

        # ---- AZ pinning ----
        affinity = {
          nodeAffinity = {
            requiredDuringSchedulingIgnoredDuringExecution = {
              nodeSelectorTerms = [{
                matchExpressions = [{
                  key      = "topology.kubernetes.io/zone"
                  operator = "In"
                  values   = var.monitoring_zones
                }]
              }]
            }
          }
        }

        # ---- your ingress (unchanged, just inlined) ----
        ingress = {
          enabled          = true
          ingressClassName = "alb"
          annotations = {
            "kubernetes.io/ingress.class"               = "alb"
            "alb.ingress.kubernetes.io/scheme"          = var.prometheus_alb_scheme
            "alb.ingress.kubernetes.io/target-type"     = "ip"
            "alb.ingress.kubernetes.io/healthcheck-path"= "/login"
            "alb.ingress.kubernetes.io/success-codes"   = "200-399"
            "alb.ingress.kubernetes.io/healthcheck-port"= "traffic-port"
            "alb.ingress.kubernetes.io/group.name"      = var.monitoring_alb_group
            "alb.ingress.kubernetes.io/group.order"     = "20"
          }
          hosts    = [var.grafana_host]
          paths    = ["/"]
          pathType = "Prefix"
        }
      }

      prometheus = {
        # ---- your ingress (unchanged, just inlined) ----
        ingress = {
          enabled          = true
          ingressClassName = "alb"
          annotations = {
            "kubernetes.io/ingress.class"               = "alb"
            "alb.ingress.kubernetes.io/scheme"          = var.prometheus_alb_scheme
            "alb.ingress.kubernetes.io/target-type"     = "ip"
            "alb.ingress.kubernetes.io/healthcheck-path"= "/-/healthy"
            "alb.ingress.kubernetes.io/success-codes"   = "200"
            "alb.ingress.kubernetes.io/healthcheck-port"= "traffic-port"
            "alb.ingress.kubernetes.io/group.name"      = var.monitoring_alb_group
            "alb.ingress.kubernetes.io/group.order"     = "20"
          }
          hosts    = [var.prometheus_host]
          paths    = ["/"]
          pathType = "Prefix"
        }

        prometheusSpec = {
          retention      = var.prometheus_retention
          walCompression = true

          # ---- AZ pinning ----
          affinity = {
            nodeAffinity = {
              requiredDuringSchedulingIgnoredDuringExecution = {
                nodeSelectorTerms = [{
                  matchExpressions = [{
                    key      = "topology.kubernetes.io/zone"
                    operator = "In"
                    values   = var.monitoring_zones
                  }]
                }]
              }
            }
          }

          # ---- persistence (EBS gp3) ----
          storageSpec = {
            volumeClaimTemplate = {
              spec = {
                storageClassName = var.prometheus_storage_class
                accessModes      = ["ReadWriteOnce"]
                resources        = { requests = { storage = var.prometheus_storage_size } }
              }
            }
          }
        }
      }

      alertmanager = {
        enabled = true
        ingress = { enabled = false }

        alertmanagerSpec = {
          # ---- AZ pinning ----
          affinity = {
            nodeAffinity = {
              requiredDuringSchedulingIgnoredDuringExecution = {
                nodeSelectorTerms = [{
                  matchExpressions = [{
                    key      = "topology.kubernetes.io/zone"
                    operator = "In"
                    values   = var.monitoring_zones
                  }]
                }]
              }
            }
          }

          # ---- persistence (EBS gp3) ----
          storage = {
            volumeClaimTemplate = {
              spec = {
                storageClassName = var.alertmanager_storage_class
                accessModes      = ["ReadWriteOnce"]
                resources        = { requests = { storage = var.alertmanager_storage_size } }
              }
            }
          }
        }
      }
    }),
    file("${path.module}/arc-scrape-config.yaml"),
  ]

  depends_on = [kubectl_manifest.monitoring_namespace]
}

