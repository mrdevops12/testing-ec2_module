curl -o kubectl https://s3.us-east-1.amazonaws.com/amazon-eks/1.30.0/2024-05-17/bin/linux/amd64/kubectl
curl -LO "https://s3.us-west-2.amazonaws.com/amazon-eks/1.30.0/2024-05-17/bin/linux/amd64/kubectl"

chmod +x ./kubectl
mv ./kubectl /usr/local/bin/


curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.30.0/2024-05-17/bin/linux/amd64/aws-iam-authenticator
chmod +x ./aws-iam-authenticator
mv aws-iam-authenticator /usr/local/bin/


aws eks update-kubeconfig --name <clsuter-name> --region us-east-1



grep -A5 -B2 "client.authentication.k8s.io" ~/.kube/config
sed -i 's/client.authentication.k8s.io\/v1alpha1/client.authentication.k8s.io\/v1beta1/g' ~/.kube/config

cat ~/.kube/config | grep client.authentication.k8s


# Remove AWS CLI v1
sudo yum remove -y awscli

# Download and install AWS CLI v2
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
echo 'export PATH=/usr/local/bin:$PATH' >> ~/.bashrc
source ~/.bashrc


# Verify version
aws --version




curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

helm version

sudo yum install -y amazon-linux-extras
sudo amazon-linux-extras enable epel
sudo yum install -y helm




# 1) What PSS is enforced on the namespace?
kubectl get ns monitoring -o jsonpath='{.metadata.labels}'; echo

# You want to see:
# "pod-security.kubernetes.io/enforce":"privileged"
# (and optionally audit/warn also set to privileged)

# 2) Did Helm roll back the release?
helm -n monitoring list
helm -n monitoring status kube-prometheus-stack || true

# 3) Recent events – look for "forbidden" / "violates PodSecurity" messages
kubectl -n monitoring get events --sort-by=.metadata.creationTimestamp | tail -n 200

# 4) If any objects still exist, describe node-exporter
kubectl -n monitoring describe ds -l app.kubernetes.io/name=node-exporter || true



kubectl delete secret -n monitoring -l "owner=helm,name=kube-prometheus-stack"

kubectl get crd | grep monitoring.coreos.com
# or, with column output:
kubectl get crd -o custom-columns=NAME:.metadata.name | grep monitoring.coreos.com



kubectl get servicemonitors,podmonitors,prometheusrules,probes,alertmanagers,prometheuses,thanosrulers,alertmanagerconfigs,scrapeconfigs -A

kubectl delete servicemonitors,podmonitors,prometheusrules,probes,alertmanagers,prometheuses,thanosrulers,alertmanagerconfigs,scrapeconfigs -A --ignore-not-found



# Delete only monitoring.coreos.com CRDs
kubectl delete crd \
  alertmanagers.monitoring.coreos.com \
  alertmanagerconfigs.monitoring.coreos.com \
  prometheuses.monitoring.coreos.com \
  prometheusrules.monitoring.coreos.com \
  servicemonitors.monitoring.coreos.com \
  podmonitors.monitoring.coreos.com \
  probes.monitoring.coreos.com \
  scrapeconfigs.monitoring.coreos.com \
  thanosrulers.monitoring.coreos.com

kubectl get crd -o name | grep monitoring.coreos.com | xargs kubectl delete

kubectl delete ns monitoring

# remove the Prometheus admission secret (safe)
kubectl -n monitoring delete secret kube-prometheus-stack-admission

# if any Helm history secrets for kube-prometheus-stack remain (none shown, but just in case):
kubectl -n monitoring delete secret -l owner=helm,name=kube-prometheus-stack



# modules/prometheus_stack/main.tf (helm_release "prometheus_stack")
values = [
  yamlencode({
    grafana = {
      enabled = false
    }
    # …your other values…
  }),
  file("${path.module}/arc-scrape-config.yaml")
]



http://prometheus-kube-prometheus-stack-prometheus.monitoring.svc:9090




# controller (namespace seems to be runner-controller in your screenshot)
kubectl -n runner-controller port-forward deploy/runner-controller-gha-rs-controller 9091:8080 &
curl -s localhost:9091/metrics | grep -E '^gha_|controller_runtime_' | head

# listener (replace pod name)
kubectl -n runner-controller port-forward pod/aws-arc-ss-...-listener 9090:8080 &
curl -s localhost:9090/metrics | grep -E '^gha_|workqueue_' | head













